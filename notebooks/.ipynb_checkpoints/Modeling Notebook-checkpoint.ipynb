{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(X_train, X_test, minmax = False):\n",
    "    if minmax == True:\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def get_indices(df):\n",
    "    X_full_indices = df.drop(['pt_suc', 'pt_attempt'], axis =1 ).columns\n",
    "    return X_full_indices\n",
    "\n",
    "\n",
    "def splitter(df, target = 'pt_attempt', test_size = .25, random_state = 29, VIF_drop = False, scaled = False, minmax = False):\n",
    "    _targets = ['pt_attempt', 'pt_suc']\n",
    "    df = add_constant(df)\n",
    "    indices = get_indices(df)\n",
    "    if VIF_drop == True:\n",
    "        df = df.drop(vifdrops, axis = 1)\n",
    "        y = df[target]\n",
    "        X = df.drop(_targets, axis = 1)\n",
    "    if VIF_drop == False:\n",
    "        y = df[target]\n",
    "        X = df.drop(_targets, axis = 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test_size, random_state= random_state, stratify = y )\n",
    "    if scaled == True:\n",
    "        X_train, X_test = scaler(X_train, X_test, minmax = minmax)\n",
    "    return X_train, X_test, y_train, y_test, indices\n",
    "\n",
    "def upsampler(X_train, y_train, target = 'pt_attempt'):\n",
    "    X = pd.concat([X_train, y_train], axis=1) \n",
    "    no_coup = X[X[target]==0]\n",
    "    coup = X[X[target]==1]\n",
    "    coups_upsampled = resample(coup,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(no_coup), # match number in majority class\n",
    "                          random_state=29)\n",
    "    upsampled = pd.concat([no_coup, coups_upsampled])\n",
    "    y_up = upsampled[target]\n",
    "    X_up = upsampled.drop(target, axis = 1)\n",
    "    return X_up, y_up\n",
    "\n",
    "def downsampler(X_train, y_train, target = 'pt_attempt'):\n",
    "    X = pd.concat([X_train, y_train], axis=1) \n",
    "    no_coup = X[X[target]==0]\n",
    "    coup = X[X[target]==1]\n",
    "    coups_downsampled = resample(no_coup,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(coup), # match number in majority class\n",
    "                          random_state=29)\n",
    "    downsampled = pd.concat([coup, coups_downsampled])\n",
    "    y_down = downsampled[target]\n",
    "    X_down = downsampled.drop(target, axis = 1)\n",
    "    return X_down, y_down\n",
    "\n",
    "def smoter(X_train, y_train, ratio = 1.0):\n",
    "    sm = SMOTE(random_state=29, ratio=ratio)\n",
    "    X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)\n",
    "    return X_train_sm, y_train_sm\n",
    "\n",
    "\n",
    "def variance_inflation_factors(X):\n",
    "    # X = add_constant(X)\n",
    "    vifs = pd.Series(\n",
    "        [1 / (1. - OLS(X[col].values, \n",
    "                       X.loc[:, X.columns != col].values).fit().rsquared) \n",
    "         for col in X],\n",
    "        index=X.columns,\n",
    "        name='VIF'\n",
    "    )\n",
    "    return vifs.sort_values()\n",
    "\n",
    "#these were determined through looking at VIF outputs and removing one member of pairs that seemed correlated\n",
    "VIFdrops = ['election_recent', 'imports', 'exports', 'victory_recent', 'upop', 'direct_recent', 'leg_recent', 'pec', 'anticipation', 'cinc', 'lead_recent']\n",
    "\n",
    "\n",
    "\n",
    "def data_pipeline(df, target = 'pt_attempt', test_size = .25, random_state = 29, VIF_drop = False, scaled = False, minmax = False, resampler = None, sample_ratio = 1):\n",
    "    X_train, X_test, y_train, y_test, indices = splitter(df, target = 'pt_attempt', test_size = .25, \n",
    "                                                random_state = 29, VIF_drop = False, scaled = scaled, minmax = minmax)\n",
    "    if resampler == 'upsample':\n",
    "        X_train, y_train = upsampler(X_train, y_train)\n",
    "    if resampler == 'downsample':\n",
    "        X_train, y_train = downsampler(X_train, y_train)\n",
    "    if resampler == 'smote':\n",
    "        X_train, y_train = smoter(X_train, y_train, ratio = sample_ratio)\n",
    "    return X_train, X_test, y_train, y_test, indices\n",
    "\n",
    "def get_feature_weights(model, indices):\n",
    "    d_log_vals = {}\n",
    "    for idx, feat in enumerate(model.coef_[0]):\n",
    "        d_log_vals[indices[idx]] = feat  \n",
    "    s_log_vals = (pd.Series(d_log_vals)).sort_values()\n",
    "    return s_log_vals\n",
    "\n",
    "def metric_test(model, X_test, y_test):\n",
    "    preds = model.predict(X_test)\n",
    "    print('accuracy = ' + str(accuracy_score(y_test, preds)))\n",
    "    print('recall = ' + str(recall_score(y_test, preds)))\n",
    "    print('precision = ' + str(precision_score(y_test, preds)))\n",
    "    print('f1 score = ' + str(f1_score(y_test, preds)))\n",
    "    print('accuracy + recall = ' + str(accuracy_score(y_test, preds) + recall_score(y_test, preds)))\n",
    "\n",
    "def fit_test_model(model, X_train, X_test, y_train, y_test, indices, do_metric_test = True, get_features = False):\n",
    "    model.fit(X_train, y_train)\n",
    "    if do_metric_test == True:\n",
    "        metric_test(model, X_test, y_test)\n",
    "    if get_features == True:\n",
    "        features = get_feature_weights(model, indices)\n",
    "        print(features)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/pickles/df_one_hot_num.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106008, 59)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vifdrops = ['election_recent', 'imports', 'exports', 'victory_recent', 'upop', 'direct_recent', 'leg_recent', 'pec', 'anticipation', 'cinc', 'lead_recent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, indices = data_pipeline(df, target = 'pt_attempt', test_size = .25, random_state = 29, VIF_drop = True, scaled = True, minmax = True, resampler = 'smote', sample_ratio = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158348, 58)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_scaled = LogisticRegressionCV(\n",
    "        cv=5, dual=False,\n",
    "        penalty='elasticnet', \n",
    "        scoring='recall',\n",
    "        solver='saga', \n",
    "        n_jobs = 2,\n",
    "        tol=0.0001,\n",
    "        max_iter=100,\n",
    "        l1_ratios = [0, .5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7094181571202174\n",
      "recall = 0.8018018018018018\n",
      "precision = 0.011457260556127703\n",
      "f1 score = 0.02259169945424546\n",
      "accuracy + recall = 1.5112199589220192\n",
      "exports                          -146.069520\n",
      "irst                              -90.517136\n",
      "imports                           -74.646179\n",
      "cinc                              -38.554844\n",
      "indirect_recent                    -9.250819\n",
      "ref_ant                            -8.443522\n",
      "election_recent                    -8.423086\n",
      "leg_ant                            -7.347210\n",
      "exec_ant                           -5.866158\n",
      "ref_recent                         -5.045200\n",
      "irregular                          -4.681233\n",
      "lead_recent                        -4.383306\n",
      "year                               -1.845672\n",
      "mil_percent                        -1.033233\n",
      "precip                             -0.836847\n",
      "Foreign/Occupied                   -0.832910\n",
      "leg_recent                         -0.586812\n",
      "tenure_months                      -0.544121\n",
      "loss                               -0.499434\n",
      "Monarchy                           -0.492524\n",
      "Dominant Party                     -0.414295\n",
      "victory_recent                     -0.405979\n",
      "Oligarchy                          -0.366303\n",
      "election_now                       -0.191470\n",
      "Party-Personal                     -0.171847\n",
      "male                               -0.073597\n",
      "elected                            -0.068353\n",
      "militarycareer                     -0.066194\n",
      "irreg_lead_ant                     -0.054386\n",
      "exec_recent                        -0.046076\n",
      "milper                             -0.015917\n",
      "const                               0.000000\n",
      "month                               0.188431\n",
      "Party-Personal-Military Hybrid      0.193835\n",
      "delayed                             0.247518\n",
      "age                                 0.248084\n",
      "defeat_recent                       0.284270\n",
      "Personal Dictatorship               0.540395\n",
      "population                          0.560351\n",
      "Presidential Democracy              0.572787\n",
      "Provisional - Military              0.675697\n",
      "Party-Military                      0.923126\n",
      "Warlordism                          1.109673\n",
      "Military-Personal                   1.134096\n",
      "trade balance                       1.172000\n",
      "urban_percent                       1.191429\n",
      "Military                            1.199650\n",
      "Indirect Military                   1.202600\n",
      "Provisional - Civilian              1.756907\n",
      "prev_conflict                       2.101917\n",
      "lastelection                        4.302876\n",
      "nochange_recent                     5.074632\n",
      "change_recent                       5.168068\n",
      "anticipation                        6.715666\n",
      "direct_recent                       8.558606\n",
      "milex                              11.783931\n",
      "upop                               34.164815\n",
      "pec                                49.268589\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylorbritt/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0,\n",
       "                     l1_ratios=[0, 0.5, 1], max_iter=100, multi_class='warn',\n",
       "                     n_jobs=2, penalty='elasticnet', random_state=None,\n",
       "                     refit=True, scoring='recall', solver='saga', tol=0.0001,\n",
       "                     verbose=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test_model(ridge_scaled, X_train, X_test, y_train, y_test, indices, get_features = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
